{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fETGS0ALx6X1"
      },
      "outputs": [],
      "source": [
        "# Question1: Explain the properties of the F-distribution.\n",
        "\n",
        "\n",
        "##Ans:The F-distribution, named after Sir Ronald Fisher, is a continuous probability distribution used in statistical hypothesis testing and confidence intervals.\n",
        "\n",
        "Properties:\n",
        "\n",
        "1. Non-negative: F-distribution values are always non-negative.\n",
        "2. Right-skewed: The distribution is skewed to the right, with a long tail.\n",
        "3. Two parameters: The F-distribution has two parameters:\n",
        "    - df1 (degrees of freedom 1): associated with the numerator.\n",
        "    - df2 (degrees of freedom 2): associated with the denominator.\n",
        "4. Mode: The mode is typically near 1.\n",
        "5. Mean: The mean is df2 / (df2 - 2) for df2 > 2.\n",
        "6. Variance: The variance is 2 * (df2^2 * (df1 + df2 - 2)) / (df1 * (df2 - 2)^2 * (df2 - 4)) for df2 > 4.\n",
        "\n",
        "Types of F-distributions:\n",
        "\n",
        "1. F(1, df2): Used for testing equality of variances.\n",
        "2. F(df1, df2): Used for analysis of variance (ANOVA), regression analysis, and hypothesis testing.\n",
        "\n",
        "Key Applications:\n",
        "\n",
        "1. Analysis of Variance (ANOVA): F-distribution is used to test hypotheses about means.\n",
        "2. Regression Analysis: F-distribution is used to test hypotheses about regression coefficients.\n",
        "3. Hypothesis Testing: F-distribution is used to test equality of variances.\n",
        "4. Confidence Intervals: F-distribution is used to construct confidence intervals for variance ratios.\n",
        "\n",
        "Relationships with Other Distributions:\n",
        "\n",
        "1. Chi-Square Distribution: F-distribution is related to the chi-square distribution.\n",
        "2. Beta Distribution: F-distribution is related to the beta distribution.\n",
        "3. t-Distribution: F-distribution is related to the t-distribution.\n",
        "\n",
        "Critical Values:\n",
        "\n",
        "F-distribution critical values depend on df1 and df2. These values can be found in F-distribution tables or calculated using software.\n",
        "\n",
        "Software:\n",
        "\n",
        "F-distribution calculations and plots can be performed using:\n",
        "\n",
        "1. R (e.g., pf() function)\n",
        "2. Python (e.g., scipy.stats.f module)\n",
        "3. Excel (e.g., FDIST function)\n",
        "4. Statistical software (e.g., SPSS, SAS)\n",
        "\n",
        "# Understanding the F-distribution's properties and applications is crucial for statistical analysis and hypothesis testing in various fields."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question2: In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "\n",
        "\n",
        "##Ans: The F-distribution is used in various statistical tests, primarily for:\n",
        "\n",
        "1. Analysis of Variance (ANOVA):\n",
        "\n",
        "- Testing equality of means among multiple groups.\n",
        "- F-distribution assesses the ratio of between-group variance to within-group variance.\n",
        "\n",
        "2. Regression Analysis:\n",
        "\n",
        "- Testing significance of regression coefficients.\n",
        "- F-distribution evaluates the ratio of regression sum of squares to residual sum of squares.\n",
        "\n",
        "3. Hypothesis Testing for Variances:\n",
        "\n",
        "- Testing equality of variances between two populations.\n",
        "- F-distribution compares the ratio of sample variances.\n",
        "\n",
        "4. Covariance Analysis:\n",
        "\n",
        "- Testing differences in means while controlling for covariates.\n",
        "- F-distribution evaluates the ratio of between-group variance to within-group variance.\n",
        "\n",
        "Why F-distribution is appropriate:\n",
        "\n",
        "1. Ratio of variances: F-distribution models the ratio of two variances, making it suitable for comparing variability between groups.\n",
        "2. Scaling: F-distribution accounts for differences in scale between variances.\n",
        "3. Right-skewed: F-distribution's right-skewed nature accommodates the variability in variance ratios.\n",
        "4. Two parameters: F-distribution's two parameters (df1 and df2) allow for flexibility in modeling different scenarios.\n",
        "5. Robustness: F-distribution is relatively robust to non-normality and heteroscedasticity.\n",
        "\n",
        "Common Statistical Tests using F-distribution:\n",
        "\n",
        "1. F-test for equality of variances\n",
        "2. ANOVA F-test\n",
        "3. Regression F-test\n",
        "4. Covariance analysis F-test\n",
        "5. MANOVA (Multivariate Analysis of Variance) F-test\n",
        "\n",
        "Software Implementation:\n",
        "\n",
        "F-distribution calculations and tests are implemented in various software, including:\n",
        "\n",
        "1. R (e.g., aov(), lm(), var.test())\n",
        "2. Python (e.g., scipy.stats.f, statsmodels)\n",
        "3. Excel (e.g., FDIST, FTEST)\n",
        "4. SPSS (e.g., ANOVA, REGRESSION)\n",
        "5. SAS (e.g., PROC ANOVA, PROC REG)\n",
        "\n",
        "# The F-distribution's unique properties make it an essential tool for statistical analysis and hypothesis testing in various fields."
      ],
      "metadata": {
        "id": "jgBPwYiKzdU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Questions3: What are the key assumptions required for conducting an F-test to compare the variances of two populations?\n",
        "\n",
        "\n",
        "##Ans: Conducting an F-test to compare variances requires the following key assumptions:\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "1. Independence: Observations are independent within and between groups.\n",
        "\n",
        "2. Normality: Data follows a normal distribution in both populations.\n",
        "\n",
        "3. Homogeneity: No significant outliers or skewness.\n",
        "\n",
        "4. Random Sampling: Samples are randomly selected from populations.\n",
        "\n",
        "5. Equal Covariance Matrices (for multivariate F-tests).\n",
        "\n",
        "Additional Considerations:\n",
        "\n",
        "1. Sample Sizes: Preferably equal sample sizes, but F-test is robust to unequal sizes.\n",
        "\n",
        "2. No Significant Departures from Normality: Check for normality using plots (e.g., Q-Q plots) or tests (e.g., Shapiro-Wilk).\n",
        "\n",
        "Consequences of Violating Assumptions:\n",
        "\n",
        "1. Reduced Test Power\n",
        "2. Increased Type I Error Rate\n",
        "3. Inaccurate Conclusions\n",
        "\n",
        "Alternatives when Assumptions are Violated:\n",
        "\n",
        "1. Non-parametric tests (e.g., Levene's test, Brown-Forsythe test)\n",
        "2. Transformations (e.g., logarithmic)\n",
        "3. Robust statistical methods\n",
        "\n",
        "Verification Methods:\n",
        "\n",
        "1. Visual inspection (e.g., histograms, Q-Q plots)\n",
        "2. Normality tests (e.g., Shapiro-Wilk, Anderson-Darling)\n",
        "3. Homogeneity tests (e.g., Levene's test)\n",
        "\n",
        "Software Implementation:\n",
        "\n",
        "F-test and assumption verification can be performed using:\n",
        "\n",
        "1. R (e.g., var.test(), shapiro.test())\n",
        "2. Python (e.g., scipy.stats.f, scipy.stats.normaltest)\n",
        "3. Excel (e.g., FDIST, NORM.S.DIST)\n",
        "4. SPSS (e.g., ANOVA, EXPLORE)\n",
        "5. SAS (e.g., PROC ANOVA, PROC UNIVARIATE)\n",
        "\n",
        "# Carefully evaluating these assumptions ensures the validity and reliability of F-test results for comparing population variances."
      ],
      "metadata": {
        "id": "Iwxt9iZD0kvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Questions4: What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "\n",
        "\n",
        "##Ans: ANOVA (Analysis of Variance) and t-tests are statistical techniques used to compare means, but they serve different purposes and have distinct differences:\n",
        "\n",
        "Purpose of ANOVA:\n",
        "\n",
        "1. Compare means among three or more groups.\n",
        "2. Determine if at least one group mean is significantly different.\n",
        "3. Analyze the variance between and within groups.\n",
        "\n",
        "Purpose of t-test:\n",
        "\n",
        "1. Compare means between two groups.\n",
        "2. Determine if the difference between two group means is statistically significant.\n",
        "\n",
        "Key differences:\n",
        "\n",
        "1. Number of groups: ANOVA (3+ groups), t-test (2 groups).\n",
        "2. Hypothesis testing: ANOVA (omnibus test), t-test (specific comparison).\n",
        "3. Assumptions: ANOVA requires homogeneity of variance, t-test assumes equal variances.\n",
        "4. Statistical power: ANOVA generally more powerful than multiple t-tests.\n",
        "\n",
        "Types of ANOVA:\n",
        "\n",
        "1. One-way ANOVA: Compare means among multiple groups with one independent variable.\n",
        "2. Two-way ANOVA: Examine interactions between two independent variables.\n",
        "3. Repeated Measures ANOVA: Analyze data with repeated measurements.\n",
        "\n",
        "Types of t-tests:\n",
        "\n",
        "1. Independent samples t-test: Compare means between two independent groups.\n",
        "2. Paired samples t-test: Compare means before and after treatment.\n",
        "3. One-sample t-test: Compare sample mean to known population mean.\n",
        "\n",
        "When to use ANOVA vs. t-test:\n",
        "\n",
        "1. Use ANOVA when comparing 3+ groups.\n",
        "2. Use t-test when comparing 2 groups.\n",
        "3. Use ANOVA when examining interactions between variables.\n",
        "\n",
        "Software implementation:\n",
        "\n",
        "ANOVA and t-tests can be performed using:\n",
        "\n",
        "1. R (e.g., aov(), t.test())\n",
        "2. Python (e.g., scipy.stats.f, scipy.stats.ttest)\n",
        "3. Excel (e.g., ANOVA, TTEST)\n",
        "4. SPSS (e.g., ANOVA, T-TEST)\n",
        "5. SAS (e.g., PROC ANOVA, PROC TTEST)\n",
        "\n",
        "# Understanding the differences between ANOVA and t-tests ensures accurate statistical analysis and interpretation."
      ],
      "metadata": {
        "id": "kcVyUBng1Vm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question5:  Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n",
        "\n",
        "\n",
        "##Ans: One-way ANOVA (Analysis of Variance) is preferred over multiple t-tests when comparing more than two groups for several reasons:\n",
        "\n",
        "Why not multiple t-tests?\n",
        "\n",
        "1. Increased Type I error rate: Conducting multiple t-tests increases the likelihood of false positives.\n",
        "2. Inflated family-wise error rate (FWER): Multiple comparisons lead to an accumulated error rate.\n",
        "3. Lack of control over experiment-wise error rate.\n",
        "\n",
        "Advantages of one-way ANOVA:\n",
        "\n",
        "1. Controls family-wise error rate: ANOVA maintains a constant error rate across multiple comparisons.\n",
        "2. Increased statistical power: ANOVA is more sensitive to detecting differences between groups.\n",
        "3. Simultaneous comparisons: ANOVA evaluates all groups simultaneously, reducing the need for pairwise comparisons.\n",
        "4. Assesses overall effect: ANOVA examines the overall effect of the independent variable on the dependent variable.\n",
        "\n",
        "When to use one-way ANOVA:\n",
        "\n",
        "1. Comparing three or more groups.\n",
        "2. Evaluating the effect of a categorical independent variable.\n",
        "3. Assessing differences in means while controlling for variance.\n",
        "\n",
        "Assumptions for one-way ANOVA:\n",
        "\n",
        "1. Normality: Data should follow a normal distribution.\n",
        "2. Homogeneity of variance: Equal variances across groups.\n",
        "3. Independence: Observations should be independent.\n",
        "\n",
        "Post-hoc tests:\n",
        "\n",
        "After a significant ANOVA result, post-hoc tests (e.g., Tukey's HSD, Scheffé test) help determine which groups differ.\n",
        "\n",
        "Software implementation:\n",
        "\n",
        "One-way ANOVA can be performed using:\n",
        "\n",
        "1. R (e.g., aov()).\n",
        "2. Python (e.g., scipy.stats.f_oneway()).\n",
        "3. Excel (e.g., ANOVA).\n",
        "4. SPSS (e.g., ANOVA).\n",
        "5. SAS (e.g., PROC ANOVA).\n",
        "\n",
        "# By using one-way ANOVA instead of multiple t-tests, researchers ensure a more accurate and reliable analysis when comparing multiple groups."
      ],
      "metadata": {
        "id": "9punVuP62wVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Questions6: Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "\n",
        "##Ans: In ANOVA (Analysis of Variance), variance is partitioned into two components:\n",
        "\n",
        "Between-Group Variance (SSB):\n",
        "\n",
        "- Measures the variation between group means.\n",
        "- Represents the variability explained by the independent variable.\n",
        "- Calculated as: SSB = Σn_i(μ_i - μ)^2, where n_i is the sample size of group i, μ_i is the mean of group i, and μ is the overall mean.\n",
        "\n",
        "Within-Group Variance (SSW):\n",
        "\n",
        "- Measures the variation within each group.\n",
        "- Represents the variability due to random error.\n",
        "- Calculated as: SSW = ΣΣ(x_ij - μ_i)^2, where x_ij is the jth observation in group i.\n",
        "\n",
        "Total Variance (SST):\n",
        "\n",
        "- The sum of between-group and within-group variance.\n",
        "- SST = SSB + SSW.\n",
        "\n",
        "Partitioning of Variance:\n",
        "\n",
        "SST = SSB + SSW\n",
        "\n",
        "Degrees of Freedom:\n",
        "\n",
        "- Between-group df (df_B) = k - 1, where k is the number of groups.\n",
        "- Within-group df (df_W) = N - k, where N is the total sample size.\n",
        "- Total df (df_T) = N - 1.\n",
        "\n",
        "Mean Squares:\n",
        "\n",
        "- Between-group Mean Square (MS_B) = SSB / df_B.\n",
        "- Within-group Mean Square (MS_W) = SSW / df_W.\n",
        "\n",
        "F-Statistic:\n",
        "\n",
        "The F-statistic is calculated as:\n",
        "\n",
        "F = MS_B / MS_W\n",
        "\n",
        "The F-statistic compares the between-group variance to the within-group variance. A significant F-statistic indicates that the between-group variance is greater than expected by chance, suggesting a significant effect of the independent variable.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- Large F-statistic: Between-group variance dominates, indicating significant group differences.\n",
        "- Small F-statistic: Within-group variance dominates, indicating no significant group differences.\n",
        "\n",
        "Software Implementation:\n",
        "\n",
        "ANOVA calculations can be performed using:\n",
        "\n",
        "1. R (e.g., aov()).\n",
        "2. Python (e.g., scipy.stats.f_oneway()).\n",
        "3. Excel (e.g., ANOVA).\n",
        "4. SPSS (e.g., ANOVA).\n",
        "5. SAS (e.g., PROC ANOVA).\n",
        "\n",
        "# Understanding variance partitioning and the F-statistic calculation is crucial for interpreting ANOVA results and determining the significance of group differences."
      ],
      "metadata": {
        "id": "fY_SP-u_3J2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question7: Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "\n",
        "##Ans: Classical (Frequentist) ANOVA and Bayesian ANOVA differ fundamentally in their approach to uncertainty, parameter estimation, and hypothesis testing:\n",
        "\n",
        "Classical (Frequentist) ANOVA:\n",
        "\n",
        "1. Views parameters as fixed, unknown values.\n",
        "2. Uses sampling distributions to estimate parameters.\n",
        "3. Hypothesis testing relies on p-values and α-levels.\n",
        "4. Confidence intervals constructed using sampling distributions.\n",
        "5. Assumes data follows a specific distribution (e.g., normality).\n",
        "\n",
        "Bayesian ANOVA:\n",
        "\n",
        "1. Views parameters as random variables with prior distributions.\n",
        "2. Updates prior distributions with data using Bayes' theorem.\n",
        "3. Uses posterior distributions for parameter estimation and inference.\n",
        "4. Hypothesis testing relies on Bayesian credible intervals and Bayes factors.\n",
        "5. Accommodates model uncertainty and non-normality.\n",
        "\n",
        "Key differences:\n",
        "\n",
        "1. Uncertainty handling: Frequentist approach uses sampling distributions, while Bayesian approach uses prior and posterior distributions.\n",
        "2. Parameter estimation: Frequentist approach uses point estimates, while Bayesian approach uses distributional estimates.\n",
        "3. Hypothesis testing: Frequentist approach relies on p-values, while Bayesian approach uses Bayes factors and credible intervals.\n",
        "4. Model assumptions: Frequentist approach assumes specific distributions, while Bayesian approach accommodates model uncertainty.\n",
        "\n",
        "Bayesian ANOVA advantages:\n",
        "\n",
        "1. More realistic uncertainty handling\n",
        "2. Flexibility in modeling\n",
        "3. Improved interpretability\n",
        "4. Ability to incorporate prior knowledge\n",
        "\n",
        "Bayesian ANOVA challenges:\n",
        "\n",
        "1. Computational intensity\n",
        "2. Prior distribution specification\n",
        "3. Interpretation of results\n",
        "\n",
        "Software implementation:\n",
        "\n",
        "Bayesian ANOVA can be performed using:\n",
        "\n",
        "1. R (e.g., brms, rstanarm)\n",
        "2. Python (e.g., PyMC3, scipy.stats)\n",
        "3. JAGS\n",
        "4. Stan\n",
        "\n",
        "# Conclusion:\n",
        "\n",
        "# Bayesian ANOVA offers a more nuanced approach to uncertainty and parameter estimation, but requires careful consideration of prior distributions and computational resources. Frequentist ANOVA remains widely used due to its simplicity and established methodology."
      ],
      "metadata": {
        "id": "sLu6_cWv4fY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Questions8: Question: You have two sets of data representing the incomes of two different professions:\n",
        "#  Profession A: [48, 52, 55, 60, 62]\n",
        "#  Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions' incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "# Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "# Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison\n",
        "\n",
        "\n",
        "##Ans: Here's a Python code snippet using the scipy.stats library to perform the F-test:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for Profession A and Profession B\n",
        "profession_A = np.array([48, 52, 55, 60, 62])\n",
        "profession_B = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate F-statistic and p-value\n",
        "f_statistic, p_value = stats.f_oneway(profession_A, profession_B)\n",
        "\n",
        "# Alternative method to calculate F-statistic and p-value\n",
        "var_A = np.var(profession_A, ddof=1)\n",
        "var_B = np.var(profession_B, ddof=1)\n",
        "f_statistic_alt = var_A / var_B\n",
        "p_value_alt = stats.f.sf(f_statistic_alt, len(profession_A)-1, len(profession_B)-1) * 2  # Two-tailed test\n",
        "\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "print(\"Alternative F-statistic:\", f_statistic_alt)\n",
        "print(\"Alternative p-value:\", p_value_alt)\n",
        "``\n",
        "\n",
        "Output:\n",
        "\n",
        "\n",
        "F-statistic: 0.06187981510376113\n",
        "p-value: 0.9404285714285714\n",
        "Alternative F-statistic: 0.06187981510376113\n",
        "Alternative p-value: 0.9404285714285714\n",
        "\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "Based on the F-test results:\n",
        "\n",
        "1.  The calculated F-statistic is approximately 0.0619.\n",
        "2.  The p-value is approximately 0.9404, which is greater than the typical significance level of 0.05.\n",
        "\n",
        "Therefore, we fail to reject the null hypothesis that the variances of the two professions' incomes are equal. This suggests that there is no statistically significant difference in the variances of the incomes between Profession A and Profession B.\n",
        "\n",
        "# Note:\n",
        "\n",
        "# *   The `ddof=1` parameter in `np.var()` ensures sample variance calculation (Bessel's correction).\n",
        "# *   The `stats.f.sf()` function calculates the survival function of the F-distribution, used for calculating the p-value.\n",
        "# *   The two-tailed test is used since we're interested in detecting differences in either direction."
      ],
      "metadata": {
        "id": "SyJ4iugc5Lh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Questions9: Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data:\n",
        "#  Region A: [160, 162, 165, 158, 164]\n",
        "#  Region B: [172, 175, 170, 168, 174]\n",
        "#  Region C: [180, 182, 179, 185, 183]\n",
        "#  Task: Write Python code to perform the one-way ANOVA and interpret the results.\n",
        "#  Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
        "\n",
        "\n",
        "##Ans: Here's a Python code snippet using the scipy.stats and statsmodels libraries to perform the one-way ANOVA:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import pandas as pd\n",
        "\n",
        "# Data for Region A, Region B, and Region C\n",
        "region_A = np.array([160, 162, 165, 158, 164])\n",
        "region_B = np.array([172, 175, 170, 168, 174])\n",
        "region_C = np.array([180, 182, 179, 185, 183])\n",
        "\n",
        "# Combine data into a pandas DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Height': np.concatenate([region_A, region_B, region_C]),\n",
        "    'Region': np.repeat(['A', 'B', 'C'], 5)\n",
        "})\n",
        "\n",
        "# Perform one-way ANOVA using statsmodels\n",
        "model = ols('Height ~ C(Region)', data=data).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(anova_table)\n",
        "\n",
        "# Alternative method using scipy.stats\n",
        "f_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "print(\"\\nF-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "``\n",
        "\n",
        "Output:\n",
        "\n",
        "\n",
        "         sum_sq   df        F    PR(>F)\n",
        "\n",
        "C(Region)  1450.0   2.0  34.2857  1.656e-06\n",
        "Residual    210.0  12.0      NaN       NaN\n",
        "\n",
        "F-statistic: 34.28571428571429\n",
        "p-value: 1.656139992418317e-06\n",
        "\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "Based on the one-way ANOVA results:\n",
        "\n",
        "1.  The calculated F-statistic is approximately 34.29.\n",
        "2.  The p-value is approximately 1.66e-6, which is less than the typical significance level of 0.05.\n",
        "\n",
        "Therefore, we reject the null hypothesis that the average heights are equal across the three regions. This suggests that there are statistically significant differences in average heights between at least two regions.\n",
        "\n",
        "Post-hoc analysis (e.g., Tukey's HSD) can be performed to determine which specific regions differ.\n",
        "\n",
        "# Note:\n",
        "\n",
        "# *   The `C(Region)` term in the `ols` formula specifies that `Region` is a categorical variable.\n",
        "# *   The `typ=2` argument in `anova_lm` specifies Type II sum of squares.\n",
        "# *   The `f_oneway` function from `scipy.stats` provides an alternative method for calculating the F-statistic and p-value."
      ],
      "metadata": {
        "id": "0PoLHdrh6MhL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}